{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyP0G8mP50TI8U2yzw0zpCjN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9bb0578795784f99b4e0e65055a9f31f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_79a47c4cb3c040f1b75c6329f6c4bb84",
              "IPY_MODEL_159317f00fa248c2826ba39837b04e4f",
              "IPY_MODEL_f40269e198444525bbc662e74afc1a39"
            ],
            "layout": "IPY_MODEL_df3cc3b6a6934f6fa543861b74e93cf6"
          }
        },
        "79a47c4cb3c040f1b75c6329f6c4bb84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b89adb7d7734fd496967871169492e0",
            "placeholder": "​",
            "style": "IPY_MODEL_3cf66e0813c14e7fb99ecd480f7ec8e1",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "159317f00fa248c2826ba39837b04e4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_640e01b3c91d497a837f04f1d913f8e5",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5680e2b12c2e4d4fae3aebf98c643b01",
            "value": 3
          }
        },
        "f40269e198444525bbc662e74afc1a39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3601db8a2e95477e8d5caab1408d7cf8",
            "placeholder": "​",
            "style": "IPY_MODEL_f0ffdc366305483d96e5bf8411b06204",
            "value": " 3/3 [01:23&lt;00:00, 27.31s/it]"
          }
        },
        "df3cc3b6a6934f6fa543861b74e93cf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b89adb7d7734fd496967871169492e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cf66e0813c14e7fb99ecd480f7ec8e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "640e01b3c91d497a837f04f1d913f8e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5680e2b12c2e4d4fae3aebf98c643b01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3601db8a2e95477e8d5caab1408d7cf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0ffdc366305483d96e5bf8411b06204": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yojuna/local_llm_RAG/blob/main/schrodinger_what_is_life_mistral_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notebook running a local Mistral 7b Instruct Model, chained with Retrieval Augmented Generation (RAG), for conversing with the legendary collection of essays, in Erwin Schrödinger's What Is Life?"
      ],
      "metadata": {
        "id": "ZnPoOfBah3Ql"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GjfhKYMWESnE"
      },
      "outputs": [],
      "source": [
        "# colab autoreload\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Installation/setup\n",
        "\n",
        "# Reading in PDF Files\n",
        "!pip install -q -U pypdf\n",
        "# Setting Up Vector Store\n",
        "!pip install -q -U chromadb\n",
        "# Using Llama-7b-GPTQ LLM model in HuggingFace\n",
        "!pip install q -U torch auto-gptq transformers optimum\n",
        "# LangChain - Loading PDFs, Text Chunking, BGE Embeddings, Retrieval QA Chain\n",
        "!pip install -q -U langchain sentence_transformers\n",
        "\n",
        "!pip install -q -U torch datasets transformers tensorflow langchain playwright html2text sentence_transformers faiss-cpu\n",
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 trl==0.4.7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IEH5OwbfEjRJ",
        "outputId": "8d1f1048-ad0e-48b5-9000-209207bc6ede"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.0/509.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.7/60.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.6/105.6 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting q\n",
            "  Downloading q-2.7-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.2)\n",
            "Collecting auto-gptq\n",
            "  Downloading auto_gptq-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Collecting optimum\n",
            "  Downloading optimum-1.16.2-py3-none-any.whl (402 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.5/402.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
            "Collecting accelerate>=0.22.0 (from auto-gptq)\n",
            "  Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.16.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.1.99)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.23.5)\n",
            "Collecting rouge (from auto-gptq)\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Collecting gekko (from auto-gptq)\n",
            "  Downloading gekko-1.0.6-py3-none-any.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.4.1)\n",
            "Collecting peft>=0.5.0 (from auto-gptq)\n",
            "  Downloading peft-0.7.1-py3-none-any.whl (168 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.66.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from optimum) (15.0.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.22.0->auto-gptq) (5.9.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->optimum) (10.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge->auto-gptq) (1.16.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2023.3.post1)\n",
            "Installing collected packages: q, rouge, gekko, accelerate, peft, optimum, auto-gptq\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.21.0\n",
            "    Uninstalling accelerate-0.21.0:\n",
            "      Successfully uninstalled accelerate-0.21.0\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.4.0\n",
            "    Uninstalling peft-0.4.0:\n",
            "      Successfully uninstalled peft-0.4.0\n",
            "Successfully installed accelerate-0.26.1 auto-gptq-0.6.0 gekko-1.0.6 optimum-1.16.2 peft-0.7.1 q-2.7 rouge-1.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "accelerate",
                  "peft"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "auto-gptq 0.6.0 requires accelerate>=0.22.0, but you have accelerate 0.21.0 which is incompatible.\n",
            "auto-gptq 0.6.0 requires peft>=0.5.0, but you have peft 0.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import torch\n",
        "import torch\n",
        "\n",
        "# Import for loading PDFs from Google Drive.\n",
        "# Note: Not needed if GDrive is already mounted or we are using wget to get files from Web.\n",
        "# from google.colab import drive\n",
        "\n",
        "# Imports to read PDF and setup Chroma Vector Store\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "# Imports for LLM\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain import PromptTemplate  #, LLMChain\n",
        "\n",
        "# Imports for QA Retrieval Chain\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Import to Clenup LLM Output\n",
        "import textwrap\n",
        ""
      ],
      "metadata": {
        "id": "vdb-v0EuW-yS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel\n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_transformers import Html2TextTransformer\n",
        "from langchain.document_loaders import AsyncChromiumLoader\n",
        "\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Imports to read PDF and setup Chroma Vector Store\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.llms import HuggingFacePipeline"
      ],
      "metadata": {
        "id": "r7bifYAlEjJs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup the LLM"
      ],
      "metadata": {
        "id": "c1WoEU6ogeEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################################\n",
        "# Tokenizer\n",
        "#################################################################\n",
        "\n",
        "model_name='mistralai/Mistral-7B-Instruct-v0.2'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "#################################################################\n",
        "# bitsandbytes parameters\n",
        "#################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False\n",
        "\n",
        "#################################################################\n",
        "# Set up quantization config\n",
        "#################################################################\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "#################################################################\n",
        "# Load pre-trained config\n",
        "#################################################################\n",
        "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9bb0578795784f99b4e0e65055a9f31f",
            "79a47c4cb3c040f1b75c6329f6c4bb84",
            "159317f00fa248c2826ba39837b04e4f",
            "f40269e198444525bbc662e74afc1a39",
            "df3cc3b6a6934f6fa543861b74e93cf6",
            "0b89adb7d7734fd496967871169492e0",
            "3cf66e0813c14e7fb99ecd480f7ec8e1",
            "640e01b3c91d497a837f04f1d913f8e5",
            "5680e2b12c2e4d4fae3aebf98c643b01",
            "3601db8a2e95477e8d5caab1408d7cf8",
            "f0ffdc366305483d96e5bf8411b06204"
          ]
        },
        "id": "MZph5n_uEjDp",
        "outputId": "9c218570-9965-4398-f46c-377ddb9608cb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9bb0578795784f99b4e0e65055a9f31f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check/get the number of trainable parameters\n",
        "\n",
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
        "\n",
        "print(print_number_of_trainable_model_parameters(mistral_model))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1M9Ow1lKEi50",
        "outputId": "2cb0d06f-0332-4ee8-9077-963d891ef941"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable model parameters: 262410240\n",
            "all model parameters: 3752071168\n",
            "percentage of trainable model parameters: 6.99%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setup the LLM pipeline\n",
        "\n",
        "standalone_query_generation_pipeline = pipeline(\n",
        " model=mistral_model,\n",
        " tokenizer=tokenizer,\n",
        " task=\"text-generation\",\n",
        " temperature=0.0,\n",
        " repetition_penalty=1.1,\n",
        " return_full_text=True,\n",
        " max_new_tokens=1000,\n",
        ")\n",
        "standalone_query_generation_llm = HuggingFacePipeline(pipeline=standalone_query_generation_pipeline)\n",
        "\n",
        "response_generation_pipeline = pipeline(\n",
        " model=mistral_model,\n",
        " tokenizer=tokenizer,\n",
        " task=\"text-generation\",\n",
        " temperature=0.2,\n",
        " repetition_penalty=1.1,\n",
        " return_full_text=True,\n",
        " max_new_tokens=1000,\n",
        ")\n",
        "response_generation_llm = HuggingFacePipeline(pipeline=response_generation_pipeline)"
      ],
      "metadata": {
        "id": "EIHdrtj0FEgR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l-PcGJG4FEb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the documents/ data"
      ],
      "metadata": {
        "id": "ZwWUoAZ2ghvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the Document / Textbook\n",
        "\n",
        "## Need to run only once\n",
        "\n",
        "# ## Feynman lectures on physics\n",
        "\n",
        "# ! mkdir -p docs\n",
        "# ! wget https://antilogicalism.com/wp-content/uploads/2018/04/feynman-lectures.pdf -O docs/feynman-lectures.pdf\n",
        "\n",
        "! wget https://archive.org/download/feynman-lectures-on-physics-volumes-1-2-3-feynman-leighton-and-sands/Feynman%20Lectures%20on%20Physics%20Volumes%201%2C2%2C3%20-%20Feynman%2C%20Leighton%20and%20Sands.pdf -O docs/archive-feynman-lectures.pdf\n",
        "\n",
        "! wget http://strangebeautiful.com/other-texts/schrodinger-what-is-life-mind-matter-auto-sketches.pdf -O docs/what-is-life.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpgyLbO4Xtm_",
        "outputId": "5aef9f27-95df-4c5c-a2c8-9a6692c7cf43"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-29 18:24:03--  https://archive.org/download/feynman-lectures-on-physics-volumes-1-2-3-feynman-leighton-and-sands/Feynman%20Lectures%20on%20Physics%20Volumes%201%2C2%2C3%20-%20Feynman%2C%20Leighton%20and%20Sands.pdf\n",
            "Resolving archive.org (archive.org)... 207.241.224.2\n",
            "Connecting to archive.org (archive.org)|207.241.224.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ia902508.us.archive.org/29/items/feynman-lectures-on-physics-volumes-1-2-3-feynman-leighton-and-sands/Feynman%20Lectures%20on%20Physics%20Volumes%201%2C2%2C3%20-%20Feynman%2C%20Leighton%20and%20Sands.pdf [following]\n",
            "--2024-01-29 18:24:03--  https://ia902508.us.archive.org/29/items/feynman-lectures-on-physics-volumes-1-2-3-feynman-leighton-and-sands/Feynman%20Lectures%20on%20Physics%20Volumes%201%2C2%2C3%20-%20Feynman%2C%20Leighton%20and%20Sands.pdf\n",
            "Resolving ia902508.us.archive.org (ia902508.us.archive.org)... 207.241.228.218\n",
            "Connecting to ia902508.us.archive.org (ia902508.us.archive.org)|207.241.228.218|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 87275502 (83M) [application/pdf]\n",
            "Saving to: ‘docs/archive-feynman-lectures.pdf’\n",
            "\n",
            "docs/archive-feynma 100%[===================>]  83.23M  1.54MB/s    in 25s     \n",
            "\n",
            "2024-01-29 18:24:29 (3.28 MB/s) - ‘docs/archive-feynman-lectures.pdf’ saved [87275502/87275502]\n",
            "\n",
            "--2024-01-29 18:24:29--  http://strangebeautiful.com/other-texts/schrodinger-what-is-life-mind-matter-auto-sketches.pdf\n",
            "Resolving strangebeautiful.com (strangebeautiful.com)... 145.14.158.30\n",
            "Connecting to strangebeautiful.com (strangebeautiful.com)|145.14.158.30|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8726575 (8.3M) [application/pdf]\n",
            "Saving to: ‘docs/what-is-life.pdf’\n",
            "\n",
            "docs/what-is-life.p 100%[===================>]   8.32M  6.25MB/s    in 1.3s    \n",
            "\n",
            "2024-01-29 18:24:31 (6.25 MB/s) - ‘docs/what-is-life.pdf’ saved [8726575/8726575]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load document from directory\n",
        "\n",
        "loader = DirectoryLoader('docs/', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
        "\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "nVCgX5YgX0HY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of pages in the pdf\n",
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCdoDIB3a7ya",
        "outputId": "01d950f5-9a4c-41e0-9374-2aa1c5cceb88"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1572"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM Token Chunksize varies based on Context Window. LLaMA2 Context Window is 4096 tokens.\n",
        "# For QA want to pick larger chunk size with some overlap to get context.\n",
        "CHUNK_SIZE, CHUNK_OVERLAP = 1000, 200\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE,\n",
        "                                               chunk_overlap=CHUNK_OVERLAP)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu4xRsCbX0AE",
        "outputId": "c85d1d51-2120-4d52-c09c-4a10d4324a9a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "764"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[700]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I3Bx80kb4c5",
        "outputId": "adfdaeda-774d-4ada-ca69-6f41c78bfc9c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='MindandMatter\\n(notably intheZeeman andStarkeffects)someofthespectral\\nlinesarepolarized. Tocomplete thephysical description in\\nthisrespect, inwhichthehumaneyeisentirely insensitive,\\nyouputapolarizer (aNicolprism)inthepathofthebeam,\\nbeforedecomposing it;onslowlyrotating theNicolaroundits\\naxiscertain linesareextinguished orreduced tominimal\\nbrightness forcertainorientations oftheNicol,whichindicate\\nthedirection (orthogonal tothebeam)oftheirtotalorpartial\\npolariza tion.\\nOncethiswholetechnique isdeveloped, itcanbeextended\\nfarbeyond thevisibleregion.Thespectral linesofglowing\\nvapours arebynomeansrestricted tothevisibleregion,which\\nisnotdistinguished physically. Thelinesformlong,theoret\\xad\\nicallyinfinite series.Thewave-lengths ofeachseriesare\\nconnected byarelatively simplemathematical law,peculiar\\ntoit,thatholdsuniformly throughout theserieswithno\\ndistinctionofthatpartoftheseriesthathappens tolieinthe\\nvisibleregion.Theseseriallawswerefirstfoundempirically,', metadata={'source': 'docs/what-is-life.pdf', 'page': 172})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alternative Data sources"
      ],
      "metadata": {
        "id": "iSJt58zavfhS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Alternative: Extract data from News APIs"
      ],
      "metadata": {
        "id": "pI_Eic0zv0R4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Optional: If extracting News data using Google News API\n",
        "# google news api\n",
        "\n",
        "!pip install GoogleNews\n",
        "\n",
        "!pip install newspaper3k"
      ],
      "metadata": {
        "id": "pFfwYHgyK0PC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from GoogleNews import GoogleNews\n",
        "from newspaper import Article\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "dKV5aZa_K-HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "googlenews=GoogleNews(start='05/01/2024',end='28/01/2024')\n",
        "googlenews.search('Finance')\n",
        "result=googlenews.result()\n",
        "df=pd.DataFrame(result)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "yC3bG3A0K-A4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "8JcWsLlAK95F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get more articles by looping through\n",
        "\n",
        "NUM_SEARCH_PAGES = 10\n",
        "\n",
        "for i in range(2, NUM_SEARCH_PAGES):\n",
        "    googlenews.getpage(i)\n",
        "    result=googlenews.result()\n",
        "    df=pd.DataFrame(result)"
      ],
      "metadata": {
        "id": "8g-rizx9MPwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JTKLr6fjgOpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SyLAvb58MPcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Alternative: Extract web pages/ blog articles"
      ],
      "metadata": {
        "id": "0yy2X3V7kPWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## uncomment if smooth browser functionality is required for using chrome/firefox web drivers\n",
        "\n",
        "# !playwright install\n",
        "# !playwright install-deps"
      ],
      "metadata": {
        "id": "fy4JukEvv7kZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative\n",
        "# Inference over URL, using chromium driver\n",
        "\n",
        "\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Articles to index\n",
        "## Andrej Karpathy: Software 2.0 Article\n",
        "articles = [\"https://karpathy.medium.com/software-2-0-a64152b37c35\",]\n",
        "\n",
        "# Scrapes the blogs above\n",
        "loader = AsyncChromiumLoader(articles)\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "GIo4CJgIFEWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create embeddings and vector db"
      ],
      "metadata": {
        "id": "cmD4-tddg2YF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Retriever Embeddings - HF BGE Embeddings\n",
        "\n",
        "BGE Embeddings are at the top of the leader board on Hugging Face (https://huggingface.co/spaces/mteb/leaderboard).\n"
      ],
      "metadata": {
        "id": "Qe4OgiuchArw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BGE Embedding Model for Retrieval. Embedding Size is 768.\n",
        "model_name = \"BAAI/bge-base-en\"\n",
        "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
        "\n",
        "model_embedding = HuggingFaceBgeEmbeddings(\n",
        "                    model_name=model_name,\n",
        "                    model_kwargs={'device': 'cuda'},\n",
        "                    encode_kwargs=encode_kwargs\n",
        "                  )"
      ],
      "metadata": {
        "id": "kM9nCmqmbQ4w"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the Vector DB Store Using Chroma DB"
      ],
      "metadata": {
        "id": "Qebgj7eGbjfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Embed and store the texts\n",
        "# Supplying a persist_directory will store the embeddings on disk\n",
        "# Creating Vector Store takes ~ 2 mins\n",
        "\n",
        "persist_directory = 'db'\n",
        "\n",
        "## Here is the nmew embeddings being used\n",
        "embedding = model_embedding\n",
        "\n",
        "vectordb = Chroma.from_documents(documents=texts,\n",
        "                                 embedding=embedding,\n",
        "                                 persist_directory=persist_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmaCbx-nbQ2F",
        "outputId": "35b24b32-b167-466c-e110-2d2a03750edc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 14.6 s, sys: 20.1 ms, total: 14.6 s\n",
            "Wall time: 14.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns the Top-k chunks from vectordb. Set to 2 to check.\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 2})"
      ],
      "metadata": {
        "id": "S9XlanshbQz6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check retrieval from Chroma db"
      ],
      "metadata": {
        "id": "SAd2zIrfhJre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Approach 1: Use Query to do Similarity Search\n",
        "\n",
        "query = \"What is The Physical Basis of Consciousness?\"\n",
        "\n",
        "\n",
        "docs = vectordb.similarity_search(query)\n",
        "\n",
        "print(\"# of results: \", len(docs))\n",
        "\n",
        "# First page\n",
        "print(docs[0].page_content)\n",
        "print(\"\\n\")\n",
        "print(docs[0].metadata)\n",
        "\n",
        "# Last page of results\n",
        "print(docs[-1].page_content)\n",
        "print(\"\\n\")\n",
        "print(docs[-1].metadata)"
      ],
      "metadata": {
        "id": "96kfFL_nbt9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Approach 2: Use Embedding Vector to do Similarity Search\n",
        "embedding_vector = embedding.embed_query(query)\n",
        "\n",
        "docs = vectordb.similarity_search_by_vector(embedding_vector)\n",
        "\n",
        "print(\"# of results: \", len(docs))\n",
        "\n",
        "# First page\n",
        "print(docs[0].page_content)\n",
        "print(\"\\n\")\n",
        "print(docs[0].metadata)\n",
        "\n",
        "# Last page of results\n",
        "print(docs[-1].page_content)\n",
        "print(\"\\n\")\n",
        "print(docs[-1].metadata)"
      ],
      "metadata": {
        "id": "_PA40S1Lbt64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0NMmO4OObtwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YDJhABqtgPt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run LLM on data"
      ],
      "metadata": {
        "id": "ONzcWEIUhbCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is Consciousness?\"\n",
        "\n",
        "prompt_template=f'''[INST] <>\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
        "<>\n",
        "{prompt}[/INST]\n",
        "\n",
        "'''\n",
        "\n",
        "print(\"\\n\\n*** Generate:\")\n",
        "\n",
        "print(response_generation_pipeline(prompt_template)[0]['generated_text'])"
      ],
      "metadata": {
        "id": "Hqu6jnMzFENp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "493c9ec0-0f51-4a90-8aaf-e98ebb68dab2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*** Generate:\n",
            "[INST] <>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
            "<>\n",
            "What is Consciousness?[/INST]\n",
            "\n",
            "Consciousness refers to an individual's subjective experience of the world around them. It involves the ability to perceive, process, and respond to information from the environment, as well as having self-awareness and introspective abilities. The exact nature of consciousness and how it arises from physical processes in the brain is still a topic of ongoing research and debate among scientists and philosophers. Some theories suggest that consciousness emerges from complex neural activity, while others propose that it may be a fundamental property of the universe. Ultimately, consciousness is a multifaceted phenomenon that continues to intrigue and challenge our understanding of the human mind and the nature of reality itself.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vcxybwM8ijzU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UrYOgTCkijlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup RAG Chain\n",
        "\n",
        "RAG Chain = LLM + Retriever + Query Prompt"
      ],
      "metadata": {
        "id": "JLWTPGLzkxY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})"
      ],
      "metadata": {
        "id": "KtcqttQsijiV"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(llm=response_generation_llm,\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever,\n",
        "                                  return_source_documents=True)"
      ],
      "metadata": {
        "id": "89gF_vFoijfg"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check RAG responses"
      ],
      "metadata": {
        "id": "i6OI8SiSmoue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the author's thoughts on Determinism and Free Will?\"\n",
        "\n",
        "llm_response = qa_chain(query)\n",
        "llm_response['result'].split('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5tLFQ-Wk98u",
        "outputId": "20b29bbf-6fd4-42e3-b65c-4ed988b26920"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\" According to Schrödinger, based on the evidence presented, the space-time events in a living being which correspond to its mind's activity or any other actions are statistically deterministic. However, he does not believe that quantum indeterminacy plays a significant role biologically, except perhaps in enhancing the purely accidental character of certain events like meiosis and natural mutation. He emphasizes that this view is not in conflict with physics, as even clockwork is ultimately statistical in nature. Schrödinger also acknowledges that many scientists who have made fundamental contributions in biology have been influenced by his ideas, despite some disagreement. He laments that these insights are still ignored by a disconcertingly large proportion of people who should know better. He concludes by expressing the importance of understanding the relationship between determinism and free will, which he believes is a hard task that requires further consideration.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Is Life Based on the Laws of Physics? Give your own thoughts about this in the end.\"\n",
        "\n",
        "llm_response = qa_chain(query)\n",
        "llm_response['result'].split('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzRW5NSSk952",
        "outputId": "409885db-e4e9-4c24-d1dc-2629694a8c78"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " 'Erwin Schrödinger, in his book \"What is Life?\" discusses the idea that the laws of physics can explain the behavior of living organisms. He argues that while the laws of physics are important, they alone do not fully explain the complex behaviors exhibited by living systems. He suggests that there may be new types of physical laws that govern these unique properties.',\n",
              " '',\n",
              " 'Schrödinger uses the example of a clock to illustrate his point. While a clock follows the laws of physics, it is not purely mechanical because it requires winding and springs to keep running. Similarly, living organisms exhibit behaviors that cannot be explained solely by the known laws of physics.',\n",
              " '',\n",
              " 'He also mentions the concept of entropy, which is a measure of disorder or randomness in a system. The second law of thermodynamics states that the total entropy of a closed system always increases over time. However, living organisms maintain a certain level of order, defying this expectation. Schrödinger proposes that there might be other laws of physics that help explain how living systems manage to maintain their organization against the natural tendency towards disorder.',\n",
              " '',\n",
              " 'In summary, Erwin Schrödinger believed that while the laws of physics play an essential role in understanding living organisms, they do not provide a complete explanation. There might be new types of physical laws that help explain the unique properties of life.',\n",
              " '',\n",
              " 'Personal Thoughts:',\n",
              " \"Erwin Schrödinger's ideas about the relationship between the laws of physics and life are fascinating. His perspective challenges the notion that the laws of physics can fully explain the complexity of living organisms. Instead, he suggests that there might be additional physical laws yet to be discovered. This idea resonates with me because it highlights the ongoing nature of scientific discovery and the importance of questioning our current understanding. It also emphasizes the interconnectedness of various scientific fields and the potential for cross-disciplinary insights. Ultimately, I believe that both the known laws of physics and potential new discoveries contribute to our overall understanding of life.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG with better prompting"
      ],
      "metadata": {
        "id": "0HGp-mdOnUHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Default LLaMA-2 prompt style // taken from example that used Llama2 and not Mistral-7b\n",
        "\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<>\\n\", \"\\n<>\\n\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
        "\n",
        "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
        "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
        "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
        "    return prompt_template"
      ],
      "metadata": {
        "id": "ajtwf5cTk93P"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys_prompt = \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \"\"\"\n",
        "\n",
        "instruction = \"\"\"CONTEXT:/n/n {context}/n\n",
        "\n",
        "Question: {question}\"\"\"\n",
        "\n",
        "get_prompt(instruction, sys_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "7gsjZ71xnbZp",
        "outputId": "8834cbab-b280-4f0f-bc16-c770d5195400"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[INST]<>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \\n<>\\n\\nCONTEXT:/n/n {context}/n\\n\\nQuestion: {question}[/INST]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = get_prompt(instruction, sys_prompt)\n",
        "\n",
        "mistral_prompt = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")"
      ],
      "metadata": {
        "id": "RLplZuTEnbWw"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain_type_kwargs = {\"prompt\": mistral_prompt}"
      ],
      "metadata": {
        "id": "7RmcCKx1nbUK"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})"
      ],
      "metadata": {
        "id": "ChliuYoKnbRn"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the chain to answer questions\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=response_generation_llm,\n",
        "                                       chain_type=\"stuff\",\n",
        "                                       retriever=retriever,\n",
        "                                       chain_type_kwargs=chain_type_kwargs,\n",
        "                                       return_source_documents=True)"
      ],
      "metadata": {
        "id": "a47UljwTnr3S"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Cite sources\n",
        "def wrap_text_preserve_newlines(text, width=110):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text\n",
        "\n",
        "def process_llm_response(llm_response):\n",
        "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
        "    print('\\n\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        print(source.metadata['source'])\n",
        "\n"
      ],
      "metadata": {
        "id": "TuGhTi8znr02"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Use Prompted RAG to Answer Some contextual Questions\n",
        "\n",
        "RAG with better prompting gives us good responses. It figures out we are talking about specifically.\n",
        "\n",
        "It also provides the source chunks we used to provide the answer which makes it easier to verify the response.\n"
      ],
      "metadata": {
        "id": "MPbJpp2Hn0Qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1\n",
        "query = \"What is Entropy? Explain in detail what Schrodinger is talking about in this context.\"\n",
        "\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yj8TFXFinryV",
        "outputId": "f2844c07-79a7-43a2-971e-31f9f494bb08"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Entropy is a physical property measured in calories per degree Celsius (cal/oC) that quantifies the disorder\n",
            "or randomness of a system. In the context of Erwin Schrödinger's discussion, he explains that entropy is\n",
            "related to the statistical concept of order and disorder, revealed through investigations in statistical\n",
            "physics by Boltzmann and Gibbs. The relationship is expressed mathematically as entropy = k log D, where k is\n",
            "the Boltzmann constant and Da is a quantitative measure of atomistic disorder of the body in question.\n",
            "Schrodinger also mentions that the unit in which entropy is measured is the calorie, and he justifies this\n",
            "definition to remove entropy from the atmosphere of mystery that often surrounds it. He further discusses\n",
            "Nernst's theorem, which states that a physical system displays \"dynamical law\" or clock-work features only at\n",
            "absolute zero temperature, and quantum theory provides the rational foundation for this fact. Schrodinger also\n",
            "emphasizes the importance of the distinction between atoms being bound together by \"solidifying\" Heitler-\n",
            "London forces or not, with all atoms being bound in a solid and a molecule.\n",
            "\n",
            "\n",
            "Sources:\n",
            "docs/what-is-life.pdf\n",
            "docs/what-is-life.pdf\n",
            "docs/what-is-life.pdf\n",
            "docs/what-is-life.pdf\n",
            "docs/what-is-life.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2\n",
        "## relevant question for the Feynman Lectures textbook\n",
        "\n",
        "query = \"What is Conservation of Energy?\"\n",
        "\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c225OcApCse",
        "outputId": "ccb37ff2-5f53-40ac-dd83-d0932cf56b58"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The Conservation of Energy is a fundamental law of physics stating that the total energy of a closed system\n",
            "remains constant, regardless of the changes that occur within the system. It applies to all natural phenomena\n",
            "and has no known exceptions. Energy can exist in various forms such as gravitational, kinetic, heat, elastic,\n",
            "electrical, chemical, radiant, and nuclear energy, and their respective formulas can be added up to obtain the\n",
            "total energy of the system, which remains constant unless energy is added or removed from the system.\n",
            "\n",
            "\n",
            "Sources:\n",
            "docs/feynman-lectures.pdf\n",
            "docs/archive-feynman-lectures.pdf\n",
            "docs/feynman-lectures.pdf\n",
            "docs/archive-feynman-lectures.pdf\n",
            "docs/feynman-lectures.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_JyISpNipCm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qj9rifkwpww8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alternative approach\n",
        "\n",
        "Create PromptTemplate and LLMChain\n",
        "\n",
        "ref: [github: madhavthaker1 / llm/rag/conversational_rag.ipynb](https://github.com/madhavthaker1/llm/blob/main/rag/conversational_rag.ipynb)"
      ],
      "metadata": {
        "id": "p5AdtHYtpyxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# imports\n",
        "\n",
        "from langchain.schema import format_document\n",
        "from langchain_core.messages import get_buffer_string\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain_core.prompts.chat import ChatPromptTemplate\n",
        "\n",
        "from operator import itemgetter\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "tj0mr-q-qxP-"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_template = \"\"\"\n",
        "[INST]\n",
        "Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language, that can be used to query a Chroma DB vector index. This query will be used to retrieve documents with additional context.\n",
        "\n",
        "Let me share a couple examples that will be important.\n",
        "\n",
        "If you do not see any chat history, you MUST return the \"Follow Up Input\" as is:\n",
        "\n",
        "```\n",
        "Chat History:\n",
        "\n",
        "Follow Up Input: What is Entropy?\n",
        "Standalone Question:\n",
        "What is Entropy?\n",
        "```\n",
        "\n",
        "If this is the second question onwards, you should properly rephrase the question like this:\n",
        "\n",
        "```\n",
        "Chat History:\n",
        "Human: What is Entropy?\n",
        "AI:\n",
        "Entropy is a physical property measured in calories per degree Celsius (cal/oC) that quantifies the disorder or randomness of a system.\n",
        "\n",
        "Follow Up Input: How is it measured?\n",
        "Standalone Question:\n",
        "How is Entropy measured?\n",
        "```\n",
        "\n",
        "Now, with those examples, here is the actual chat history and input question.\n",
        "\n",
        "Chat History:\n",
        "{chat_history}\n",
        "\n",
        "Follow Up Input: {question}\n",
        "Standalone question:\n",
        "[your response here]\n",
        "[/INST]\n",
        "\"\"\"\n",
        "\n",
        "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)"
      ],
      "metadata": {
        "id": "oveNh87SqxLR"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "[INST]\n",
        "Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "[/INST]\n",
        "\"\"\"\n",
        "\n",
        "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "MK9HADVAqxDD"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
        "\n",
        "\n",
        "def _combine_documents(\n",
        "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
        "):\n",
        "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
        "    return document_separator.join(doc_strings)"
      ],
      "metadata": {
        "id": "yWFKID5RqxAe"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(\n",
        " return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
        ")\n",
        "\n",
        "# First we add a step to load memory\n",
        "# This adds a \"memory\" key to the input object\n",
        "loaded_memory = RunnablePassthrough.assign(\n",
        "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
        ")\n",
        "# Now we calculate the standalone question\n",
        "standalone_question = {\n",
        "    \"standalone_question\": {\n",
        "        \"question\": lambda x: x[\"question\"],\n",
        "        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
        "    }\n",
        "    | CONDENSE_QUESTION_PROMPT\n",
        "    | standalone_query_generation_llm,\n",
        "}\n",
        "# Now we retrieve the documents\n",
        "retrieved_documents = {\n",
        "    \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
        "    \"question\": lambda x: x[\"standalone_question\"],\n",
        "}\n",
        "# Now we construct the inputs for the final prompt\n",
        "final_inputs = {\n",
        "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
        "    \"question\": itemgetter(\"question\"),\n",
        "}\n",
        "# And finally, we do the part that returns the answers\n",
        "answer = {\n",
        "    \"answer\": final_inputs | ANSWER_PROMPT | response_generation_llm,\n",
        "    \"question\": itemgetter(\"question\"),\n",
        "    \"context\": final_inputs[\"context\"]\n",
        "}\n",
        "# And now we put it all together!\n",
        "final_chain = loaded_memory | standalone_question | retrieved_documents | answer"
      ],
      "metadata": {
        "id": "QocPoyw8qw9_"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_conversational_rag(question, chain, memory):\n",
        "    \"\"\"\n",
        "    Calls a conversational RAG (Retrieval-Augmented Generation) model to generate an answer to a given question.\n",
        "\n",
        "    This function sends a question to the RAG model, retrieves the answer, and stores the question-answer pair in memory\n",
        "    for context in future interactions.\n",
        "\n",
        "    Parameters:\n",
        "    question (str): The question to be answered by the RAG model.\n",
        "    chain (LangChain object): An instance of LangChain which encapsulates the RAG model and its functionality.\n",
        "    memory (Memory object): An object used for storing the context of the conversation.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary containing the generated answer from the RAG model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Prepare the input for the RAG model\n",
        "    inputs = {\"question\": question}\n",
        "\n",
        "    # Invoke the RAG model to get an answer\n",
        "    result = chain.invoke(inputs)\n",
        "\n",
        "    # Save the current question and its answer to memory for future context\n",
        "    memory.save_context(inputs, {\"answer\": result[\"answer\"]})\n",
        "\n",
        "    # Return the result\n",
        "    return result"
      ],
      "metadata": {
        "id": "mkkWwR7MrAcq"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ask yer' questions"
      ],
      "metadata": {
        "id": "Pe1nVKKGukZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial Question\n",
        "\n",
        "question = \"What is the Hereditary Mechanism?\"\n",
        "\n",
        "result = call_conversational_rag(question, final_chain, memory)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5ps25MyrAZg",
        "outputId": "7d316578-2d4f-4641-ae7a-8cf16fd20275"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'answer': 'The process by which genetic information is transmitted from parents to their offspring involves the separation of chromosomes during meiosis in the mutant organism. This results in the creation of gametes, each carrying one copy of the altered chromosome. When these gametes combine during fertilization, the resulting zygote contains one altered chromosome from each parent. This leads to the expression of the mutation in the offspring. Mutations are inherited as perfectly as the original, unchanged characteristics were, making them a change in the hereditary treasure that must be accounted for by some change in the hereditary substance. Most important breeding experiments have revealed the mechanism of heredity through careful analysis of the offspring obtained by crossing mutated individuals with non-mutated or differently mutated ones.', 'question': 'What is the process by which genetic information is transmitted from parents to their offspring?', 'context': \"separation ofthetwochromosomes onmeiosisinthemutant\\xad\\nasshown,veryschematically, inFig.g.Thisisa'pedigree',\\nrepresenting everyindividual (ofthreeconsecutive genera\\xad\\ntions)simplybythepairofchromosomes inquestion. Please\\nrealizethatifthemutanthadbothitschromosomes affected,\\nallthechildren wouldreceivethesame(mixed) inheritance,\\ndifferent fromthatofeitherparent.\\nButexperimenting inthisdomain isnotassimpleaswould\\nappearfromwhathasjustbeensaid.Itiscomplicated bythe\\nsecondimportant fact,viz.thatmutations areveryoften\\nlatent.Whatdoesthatmean?\\nInthemutant thetwo 'copies ofthecode-script' areno\\n\\nERWIN SCHRODINGER\\nFig.g.Inheritance ofamutation. Thestraight linesacrossindicate the\\ntransferofachromosome, thedoubleonesthatofthemutated chromosome.\\nTheunaccounted-for chromosomes ofthethirdgeneration comefromthe\\nmatesofthesecondgeneration, whicharenotincluded inthediagram.\\nTheyaresupposed tobenon-relatives, freeofthemutation.\\nAndsoitis.Itisimportant tostatethatweknowdefinitely\\nthatitisachangeinonechromosome only,butnotinthe\\ncorresponding 'locus'ofthehomologous chromosome. Fig.8\\nindicates thisschematically, thecrossdenoting themutated\\nlocus.Thefactthatonlyonechromosome isaffected is\\nrevealed whenthemutated individual (oftencalled'mutant')\\niscrossed withanon-mutated one.Forexactlyhalfofthe\\noffspring exhibitthemutantcharacter andhalfthenormal\\none.Thatiswhatistobeexpected asaconsequence ofthe\\nseparation ofthetwochromosomes onmeiosisinthemutant\\xad\\nasshown,veryschematically, inFig.g.Thisisa'pedigree',\\nrepresenting everyindividual (ofthreeconsecutive genera\\xad\\n\\nThisagainseemstomeanpurechance, sinceitmeansthata\\nfavourable mutation increases theprospect fortheindividual\\nofsurvival andofbegetting offspring, towhichittransmits the\\nmutation inquestion. Apartfromthis,itsactivityduringits\\nlifetime seemstobebiologically irrelevan t.Fornothing ofit\\nhasaninfluence ontheoffspring: acquired properties arenot\\ninherited. Anyskillortraining attained islost,itleavesno\\ntrace,itdieswiththeindividual, itisnottransmitted. An\\nintelligent beinginthissituation wouldfindthatnature,asit\\nwere,refuseshiscollaboration -shedoesallherself,dooms\\ntheindividual toinactivity, indeedtonihilism.\\nAsyouknow,Darwin's theorywasnotthefirstsystematic\\ntheoryofevolution. Itwaspreceded bythetheoryofLamarck,\\nwhichrestsentirely ontheassumption thatanynewfeatures\\nanindividual hasacquired byspecificsurroundings orbeha\\xad\\nviourduringitslifetimebeforeprocreation canbe,andusually\\nare,passedontoitsprogeny, ifnotentirely, atleastintraces.\\nThusifananimalbylivingonrockyorsandysoilproduced\\n\\nbythehereditary substance, thechromosomes. Atfirst,\\ntherefore, itiscertainly notfixedgenetically anditisdifficult\\ntoseehowitshouldevercometobeincorporated inthe\\nhereditary treasure. Thisisanimportant problem initself.\\nForwedoknowthathabitsareinherited as,forinstance,\\nhabitsofnestbuilding inthebirds,thevarious habitsof\\ncleanliness weobserve inourdogsandcats,tomention afew\\nobvious examples. Ifthiscouldnotbeunderstood along\\northodox Darwinian lines,Darwinism wouldhavetobe\\nabandoned. Thequestion becomes ofsingular significance in\\nitsapplication toman,sincewewishtoinferthatthestriving\\nandlabouring ofamanduringhislifetime constitute an\\nintegrating contribution tothedevelopment ofthespecies, in\\nthequiteproperbiological sense.Ibelievethesituation tobe,\\nbriefly,asfollows.\\nAccording toourassumptions thebehaviour changes paral\\xad\\nlelthoseofthephysique, firstasaconsequence ofachance\\nchange inthelatter,butverysoondirecting thefurther\\nselectional mechanism intodefinite channels, because,\\n\\noldwhendeVriesfirstpublished hisdiscovery, in1902.Small\\nwonder thatittookanother generation todiscover theinti\\xad\\nmateconnection!\\nTHEY BREED TRUE, THAT IS,THEY ARE\\nPERFECTLY INHERITED\\nMutations areinherited asperfectly astheoriginal,\\nunchanged characters were.Togiveanexample, inthefirst\\ncropofbarleyconsidered aboveafewearsmightturnupwith\\nawnsconsiderably outsidetherangeofvariability shownin\\nFig.7,saywithnoawnsatall.Theymightrepresent ade\\nVriesmutation andwouldthenbreedperfectly true,thatisto\\nsay,alltheirdescendants wouldbeequallyawnless.\\nHenceamutation isdefinitely achangeinthehereditary\\ntreasure andhastobeaccounted forbysomechangeinthe\\nhereditary substance. Actually mostoftheimportant breeding\\nexperiments, whichhaverevealed tousthemechanism of\\nheredity, consisted inacareful analysis oftheoffsp'ring\\nobtained bycrossing, according toapreconceived plan,\\nmutated (or,inmanycases,multiply mutated) withnon\\xad\\nmutated orwithdifferently mutated individuals. Ontheother\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pretty print for easier reading in the notebook\n",
        "pprint(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atiw7ffLtiQA",
        "outputId": "0dddd070-5dd6-4495-b8b6-98592021deb2"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'answer': 'The process by which genetic information is transmitted from '\n",
            "           'parents to their offspring involves the separation of chromosomes '\n",
            "           'during meiosis in the mutant organism. This results in the '\n",
            "           'creation of gametes, each carrying one copy of the altered '\n",
            "           'chromosome. When these gametes combine during fertilization, the '\n",
            "           'resulting zygote contains one altered chromosome from each parent. '\n",
            "           'This leads to the expression of the mutation in the offspring. '\n",
            "           'Mutations are inherited as perfectly as the original, unchanged '\n",
            "           'characteristics were, making them a change in the hereditary '\n",
            "           'treasure that must be accounted for by some change in the '\n",
            "           'hereditary substance. Most important breeding experiments have '\n",
            "           'revealed the mechanism of heredity through careful analysis of the '\n",
            "           'offspring obtained by crossing mutated individuals with '\n",
            "           'non-mutated or differently mutated ones.',\n",
            " 'context': 'separation ofthetwochromosomes onmeiosisinthemutant\\xad\\n'\n",
            "            \"asshown,veryschematically, inFig.g.Thisisa'pedigree',\\n\"\n",
            "            'representing everyindividual (ofthreeconsecutive genera\\xad\\n'\n",
            "            'tions)simplybythepairofchromosomes inquestion. Please\\n'\n",
            "            'realizethatifthemutanthadbothitschromosomes affected,\\n'\n",
            "            'allthechildren wouldreceivethesame(mixed) inheritance,\\n'\n",
            "            'different fromthatofeitherparent.\\n'\n",
            "            'Butexperimenting inthisdomain isnotassimpleaswould\\n'\n",
            "            'appearfromwhathasjustbeensaid.Itiscomplicated bythe\\n'\n",
            "            'secondimportant fact,viz.thatmutations areveryoften\\n'\n",
            "            'latent.Whatdoesthatmean?\\n'\n",
            "            \"Inthemutant thetwo 'copies ofthecode-script' areno\\n\"\n",
            "            '\\n'\n",
            "            'ERWIN SCHRODINGER\\n'\n",
            "            'Fig.g.Inheritance ofamutation. Thestraight linesacrossindicate '\n",
            "            'the\\n'\n",
            "            'transferofachromosome, thedoubleonesthatofthemutated chromosome.\\n'\n",
            "            'Theunaccounted-for chromosomes ofthethirdgeneration comefromthe\\n'\n",
            "            'matesofthesecondgeneration, whicharenotincluded inthediagram.\\n'\n",
            "            'Theyaresupposed tobenon-relatives, freeofthemutation.\\n'\n",
            "            'Andsoitis.Itisimportant tostatethatweknowdefinitely\\n'\n",
            "            'thatitisachangeinonechromosome only,butnotinthe\\n'\n",
            "            \"corresponding 'locus'ofthehomologous chromosome. Fig.8\\n\"\n",
            "            'indicates thisschematically, thecrossdenoting themutated\\n'\n",
            "            'locus.Thefactthatonlyonechromosome isaffected is\\n'\n",
            "            \"revealed whenthemutated individual (oftencalled'mutant')\\n\"\n",
            "            'iscrossed withanon-mutated one.Forexactlyhalfofthe\\n'\n",
            "            'offspring exhibitthemutantcharacter andhalfthenormal\\n'\n",
            "            'one.Thatiswhatistobeexpected asaconsequence ofthe\\n'\n",
            "            'separation ofthetwochromosomes onmeiosisinthemutant\\xad\\n'\n",
            "            \"asshown,veryschematically, inFig.g.Thisisa'pedigree',\\n\"\n",
            "            'representing everyindividual (ofthreeconsecutive genera\\xad\\n'\n",
            "            '\\n'\n",
            "            'Thisagainseemstomeanpurechance, sinceitmeansthata\\n'\n",
            "            'favourable mutation increases theprospect fortheindividual\\n'\n",
            "            'ofsurvival andofbegetting offspring, towhichittransmits the\\n'\n",
            "            'mutation inquestion. Apartfromthis,itsactivityduringits\\n'\n",
            "            'lifetime seemstobebiologically irrelevan t.Fornothing ofit\\n'\n",
            "            'hasaninfluence ontheoffspring: acquired properties arenot\\n'\n",
            "            'inherited. Anyskillortraining attained islost,itleavesno\\n'\n",
            "            'trace,itdieswiththeindividual, itisnottransmitted. An\\n'\n",
            "            'intelligent beinginthissituation wouldfindthatnature,asit\\n'\n",
            "            'were,refuseshiscollaboration -shedoesallherself,dooms\\n'\n",
            "            'theindividual toinactivity, indeedtonihilism.\\n'\n",
            "            \"Asyouknow,Darwin's theorywasnotthefirstsystematic\\n\"\n",
            "            'theoryofevolution. Itwaspreceded bythetheoryofLamarck,\\n'\n",
            "            'whichrestsentirely ontheassumption thatanynewfeatures\\n'\n",
            "            'anindividual hasacquired byspecificsurroundings orbeha\\xad\\n'\n",
            "            'viourduringitslifetimebeforeprocreation canbe,andusually\\n'\n",
            "            'are,passedontoitsprogeny, ifnotentirely, atleastintraces.\\n'\n",
            "            'Thusifananimalbylivingonrockyorsandysoilproduced\\n'\n",
            "            '\\n'\n",
            "            'bythehereditary substance, thechromosomes. Atfirst,\\n'\n",
            "            'therefore, itiscertainly notfixedgenetically anditisdifficult\\n'\n",
            "            'toseehowitshouldevercometobeincorporated inthe\\n'\n",
            "            'hereditary treasure. Thisisanimportant problem initself.\\n'\n",
            "            'Forwedoknowthathabitsareinherited as,forinstance,\\n'\n",
            "            'habitsofnestbuilding inthebirds,thevarious habitsof\\n'\n",
            "            'cleanliness weobserve inourdogsandcats,tomention afew\\n'\n",
            "            'obvious examples. Ifthiscouldnotbeunderstood along\\n'\n",
            "            'orthodox Darwinian lines,Darwinism wouldhavetobe\\n'\n",
            "            'abandoned. Thequestion becomes ofsingular significance in\\n'\n",
            "            'itsapplication toman,sincewewishtoinferthatthestriving\\n'\n",
            "            'andlabouring ofamanduringhislifetime constitute an\\n'\n",
            "            'integrating contribution tothedevelopment ofthespecies, in\\n'\n",
            "            'thequiteproperbiological sense.Ibelievethesituation tobe,\\n'\n",
            "            'briefly,asfollows.\\n'\n",
            "            'According toourassumptions thebehaviour changes paral\\xad\\n'\n",
            "            'lelthoseofthephysique, firstasaconsequence ofachance\\n'\n",
            "            'change inthelatter,butverysoondirecting thefurther\\n'\n",
            "            'selectional mechanism intodefinite channels, because,\\n'\n",
            "            '\\n'\n",
            "            'oldwhendeVriesfirstpublished hisdiscovery, in1902.Small\\n'\n",
            "            'wonder thatittookanother generation todiscover theinti\\xad\\n'\n",
            "            'mateconnection!\\n'\n",
            "            'THEY BREED TRUE, THAT IS,THEY ARE\\n'\n",
            "            'PERFECTLY INHERITED\\n'\n",
            "            'Mutations areinherited asperfectly astheoriginal,\\n'\n",
            "            'unchanged characters were.Togiveanexample, inthefirst\\n'\n",
            "            'cropofbarleyconsidered aboveafewearsmightturnupwith\\n'\n",
            "            'awnsconsiderably outsidetherangeofvariability shownin\\n'\n",
            "            'Fig.7,saywithnoawnsatall.Theymightrepresent ade\\n'\n",
            "            'Vriesmutation andwouldthenbreedperfectly true,thatisto\\n'\n",
            "            'say,alltheirdescendants wouldbeequallyawnless.\\n'\n",
            "            'Henceamutation isdefinitely achangeinthehereditary\\n'\n",
            "            'treasure andhastobeaccounted forbysomechangeinthe\\n'\n",
            "            'hereditary substance. Actually mostoftheimportant breeding\\n'\n",
            "            'experiments, whichhaverevealed tousthemechanism of\\n'\n",
            "            \"heredity, consisted inacareful analysis oftheoffsp'ring\\n\"\n",
            "            'obtained bycrossing, according toapreconceived plan,\\n'\n",
            "            'mutated (or,inmanycases,multiply mutated) withnon\\xad\\n'\n",
            "            'mutated orwithdifferently mutated individuals. Ontheother',\n",
            " 'question': 'What is the process by which genetic information is transmitted '\n",
            "             'from parents to their offspring?'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# follow up question with generic mention of parents\n",
        "\n",
        "question = \"What is the siginificance of the chromosomes of the parents?\"\n",
        "\n",
        "pprint(call_conversational_rag(question, final_chain, memory))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMGFCoQlseh3",
        "outputId": "4882dd6a-ecfc-4477-b324-1485cf0b3f57"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'answer': 'Chromosomes from each parent play a crucial role in the creation '\n",
            "           'of genetically diverse offspring through meiosis. During meiosis, '\n",
            "           'homologous chromosomes pair up and exchange genetic material '\n",
            "           'through a process called crossing-over. This results in the '\n",
            "           'shuffling of genes between the chromosomes from each parent, '\n",
            "           'leading to new combinations of alleles in the offspring. The '\n",
            "           'separation of these paired chromosomes during meiosis ensures that '\n",
            "           'each gamete receives one chromosome from each pair, ensuring '\n",
            "           'genetic diversity in the resulting offspring.',\n",
            " 'context': 'separation ofthetwochromosomes onmeiosisinthemutant\\xad\\n'\n",
            "            \"asshown,veryschematically, inFig.g.Thisisa'pedigree',\\n\"\n",
            "            'representing everyindividual (ofthreeconsecutive genera\\xad\\n'\n",
            "            'tions)simplybythepairofchromosomes inquestion. Please\\n'\n",
            "            'realizethatifthemutanthadbothitschromosomes affected,\\n'\n",
            "            'allthechildren wouldreceivethesame(mixed) inheritance,\\n'\n",
            "            'different fromthatofeitherparent.\\n'\n",
            "            'Butexperimenting inthisdomain isnotassimpleaswould\\n'\n",
            "            'appearfromwhathasjustbeensaid.Itiscomplicated bythe\\n'\n",
            "            'secondimportant fact,viz.thatmutations areveryoften\\n'\n",
            "            'latent.Whatdoesthatmean?\\n'\n",
            "            \"Inthemutant thetwo 'copies ofthecode-script' areno\\n\"\n",
            "            '\\n'\n",
            "            \"'statistics oflinkage' asortof'mapofproperties' withinevery\\n\"\n",
            "            'chromosome.\\n'\n",
            "            'Theseanticipations havebeenfullyconfirmed. Inthecases\\n'\n",
            "            'towhichtestshavebeenthoroughly applied (mainly, butnot\\n'\n",
            "            'only,Drosophila) thetestedproperties actually divideintoas\\n'\n",
            "            'manyseparate groups, withnolinkagefromgrouptogroup,\\n'\n",
            "            'astherearedifferent chromosomes (fourinDrosophila).\\n'\n",
            "            'Withineverygroupalinearmapofproperties canbedrawn\\n'\n",
            "            'upwhichaccounts quantitatively forthedegreeoflinkage\\n'\n",
            "            'between anytwooutofthatgroup,sothatthereislittledoubt\\n'\n",
            "            'thattheyactually arelocated,andlocatedalongaline,asthe\\n'\n",
            "            'rod-likeshapeofthechromosome suggests.\\n'\n",
            "            'Ofcourse, thescheme ofthehereditary mechanism, as\\n'\n",
            "            'drawnuphere,isstillratheremptyandcolourless, even\\n'\n",
            "            'slightly naIve.Forwehavenotsaidwhatexactlyweunder\\xad\\n'\n",
            "            'standbyaproperty. Itseemsneitheradequate norpossible to\\n'\n",
            "            \"dissectintodiscrete 'properties' thepatternofanorganism\\n\"\n",
            "            \"whichisessentially aunity,a'whole'. Now,whatweactually\\n\"\n",
            "            'stateinanyparticular caseis,thatapairofancestors were\\n'\n",
            "            '\\n'\n",
            "            'of the body by cell division (mitosis) -In mitosis every \\n'\n",
            "            'chromosome is duplicated- Reductive division (meiosis) and \\n'\n",
            "            'fertilization (syngamy) -Haploid individuals- The outstand\\xad\\n'\n",
            "            'ing relevance of the reductive division -Crossing-over. Loca\\xad\\n'\n",
            "            'tion of properties- Maximum size of a gene- Small numbers \\n'\n",
            "            '-Permanence I\\n'\n",
            "            '\\n'\n",
            "            'ERWIN SCHRODINGER\\n'\n",
            "            'Theserulesandchances areinterfered withbycrossing\\xad\\n'\n",
            "            'over.Hencetheprobability ofthiseventcanbeascertained by\\n'\n",
            "            'registering carefully thepercentage composition oftheoff\\xad\\n'\n",
            "            'springinextended breeding experiments, suitably laidoutfor\\n'\n",
            "            'thepurpose. Inanalysing thestatistics, oneaccepts the\\n'\n",
            "            \"suggestive working hypothesis thatthe'linkage' between two\\n\"\n",
            "            'properties situated inthesamechromosome, istheless\\n'\n",
            "            'frequently brokenbycrossing-over, thenearertheylietoeach\\n'\n",
            "            'other.Forthenthereislesschanceofthepointofexchange\\n'\n",
            "            'lyingbetween them,whereas properties located nearthe\\n'\n",
            "            'opposite endsofthechromosomes areseparated byevery\\n'\n",
            "            'crossing-over. (Muchthesameappliestotherecombination\\n'\n",
            "            'ofproperties locatedinhomologous chromosomes ofthesame\\n'\n",
            "            'ancestor.) Inthiswayonemayexpecttogetfromthe\\n'\n",
            "            \"'statistics oflinkage' asortof'mapofproperties' withinevery\\n\"\n",
            "            'chromosome.\\n'\n",
            "            'Theseanticipations havebeenfullyconfirmed. Inthecases\\n'\n",
            "            'towhichtestshavebeenthoroughly applied (mainly, butnot\\n'\n",
            "            '\\n'\n",
            "            'ERWIN SCHRODINGER\\n'\n",
            "            'Fig.g.Inheritance ofamutation. Thestraight linesacrossindicate '\n",
            "            'the\\n'\n",
            "            'transferofachromosome, thedoubleonesthatofthemutated chromosome.\\n'\n",
            "            'Theunaccounted-for chromosomes ofthethirdgeneration comefromthe\\n'\n",
            "            'matesofthesecondgeneration, whicharenotincluded inthediagram.\\n'\n",
            "            'Theyaresupposed tobenon-relatives, freeofthemutation.\\n'\n",
            "            'Andsoitis.Itisimportant tostatethatweknowdefinitely\\n'\n",
            "            'thatitisachangeinonechromosome only,butnotinthe\\n'\n",
            "            \"corresponding 'locus'ofthehomologous chromosome. Fig.8\\n\"\n",
            "            'indicates thisschematically, thecrossdenoting themutated\\n'\n",
            "            'locus.Thefactthatonlyonechromosome isaffected is\\n'\n",
            "            \"revealed whenthemutated individual (oftencalled'mutant')\\n\"\n",
            "            'iscrossed withanon-mutated one.Forexactlyhalfofthe\\n'\n",
            "            'offspring exhibitthemutantcharacter andhalfthenormal\\n'\n",
            "            'one.Thatiswhatistobeexpected asaconsequence ofthe\\n'\n",
            "            'separation ofthetwochromosomes onmeiosisinthemutant\\xad\\n'\n",
            "            \"asshown,veryschematically, inFig.g.Thisisa'pedigree',\\n\"\n",
            "            'representing everyindividual (ofthreeconsecutive genera\\xad',\n",
            " 'question': 'What is the role of chromosomes from each parent in the creation '\n",
            "             'of genetically diverse offspring through meiosis?'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EqI0Cdi3rEy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nerNPfEBrEva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Credits\n",
        "\n",
        "references:\n",
        "\n",
        "### Code:\n",
        "\n",
        "\n",
        "https://github.com/jai-llm/RAG_Docs_LLaMA2/blob/main/RAG_HastieBooks_chromaDB_V3.ipynb\n",
        "\n",
        "https://github.com/madhavthaker1/llm/blob/main/rag/conversational_rag.ipynb ; https://medium.com/@thakermadhav/part-2-build-a-conversational-rag-with-langchain-and-mistral-7b-6a4ebe497185\n",
        "\n",
        "https://blog.llamaindex.ai/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1\n",
        "\n",
        "### Data:\n",
        "\n",
        "http://strangebeautiful.com/other-texts/schrodinger-what-is-life-mind-matter-auto-sketches.pdf\n",
        "\n",
        "https://archive.org/details/feynman-lectures-on-physics-volumes-1-2-3-feynman-leighton-and-sands\n",
        "\n",
        "https://karpathy.medium.com/software-2-0-a64152b37c35\n"
      ],
      "metadata": {
        "id": "qP93iY8HikoZ"
      }
    }
  ]
}